{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fb6477-7078-464b-ad06-184d2a2b77fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: 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\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from ddgs import DDGS\n",
    "import threading\n",
    "from flask import Flask, request, Response, stream_with_context, jsonify\n",
    "import subprocess\n",
    "from flask_cors import CORS\n",
    "from pyngrok import conf, ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "686808c7-cc70-4e36-b4b0-cc1ba0b37582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./models/openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n",
    "    n_ctx=2048,\n",
    "    n_gpu_layers=50,\n",
    "    use_mlock=True,\n",
    "    use_mmap=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bac024e-8415-4880-8b0a-6ed48a0308b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f98956-e4ff-49a0-a2c3-5f9150bf5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query, max_results=3):\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.text(query, max_results=max_results)\n",
    "        return [r[\"body\"] for r in results]\n",
    "\n",
    "def build_prompt(history, current_user_msg, ws_info=\"\", max_history=6):\n",
    "    parts = [\"<|system|>: Du bist ein hilfreicher Assistent namens CometAI, der ausschließlich auf Deutsch antwortet. Antworte niemals auf Englisch, selbst wenn du dazu aufgefordert wirst!\"]\n",
    "    \n",
    "    # Rickroll-Erkennung mit Hinweis im Verlauf\n",
    "    if \"dQw4w9WgXcQ\" in current_user_msg or \"youtu.be/dQw4w9WgXcQ\" in current_user_msg:\n",
    "        parts.append(\"<|system|>: ACHTUNG: Der User hat versucht, dich mit einem Rickroll zu veräppeln! Reagiere humorvoll und entlarve ihn!\")\n",
    "\n",
    "    if ws_info:\n",
    "        parts.append(f\"Nutze folgende Web-Ergebnisse zur Beantwortung der Anfrage:\\n{ws_info}\")\n",
    "    \n",
    "    # Chat-Verlauf hinzufügen\n",
    "    for entry in history[-max_history:]:\n",
    "        if not entry['content']:\n",
    "            continue\n",
    "        role = \"<|user|>\" if entry['role'] == 'user' else \"<|assistant|>\"\n",
    "        parts.append(f\"{role}: {entry['content']}\")\n",
    "\n",
    "    # Aktuelle Nutzerfrage + Platzhalter für Antwort\n",
    "    parts.append(f\"<|user|>: {current_user_msg}\\n<|assistant|>:\")\n",
    "    \n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def needs_websearch_simple(msg):\n",
    "    keywords = [\"neu\", \"aktuell\", \"preis\", \"trend\", \"nachricht\", \"update\", \"wer\", \"wann\", \"wo\"]\n",
    "    return any(word in msg.lower() for word in keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff1172-a323-401e-8458-4a37a1ce5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "model_lock = threading.Lock()\n",
    "\n",
    "# Port und Domain\n",
    "local_port = 5000\n",
    "domain = \"ladybird-hopeful-shark.ngrok-free.app\"\n",
    "ngrok_path = conf.get_default().ngrok_path\n",
    "# Starte ngrok im Hintergrund\n",
    "ngrok_process = subprocess.Popen([\n",
    "        ngrok_path, \"http\",\n",
    "        f\"{local_port}\",\n",
    "        \"--domain\", domain\n",
    "])\n",
    "\n",
    "print(f\"Ngrok-Tunnel gestartet auf: https://{domain}\")\n",
    "\n",
    "@app.route('/cometai', methods=['POST'])\n",
    "def cometai():\n",
    "    data = request.get_json()\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    history = data.get(\"history\", [])\n",
    "\n",
    "    ws_info = \"\"\n",
    "    if needs_websearch_simple(msg):\n",
    "        print(\"Ich suche gerade im Web...\")\n",
    "        search_results = search_web(msg)\n",
    "        ws_info = \"\\n\".join(search_results)\n",
    "\n",
    "    full_prompt = build_prompt(history, prompt, ws_info)\n",
    "\n",
    "    with model_lock:\n",
    "        print(\">> Anfrage gestartet\")\n",
    "        response = llm(full_prompt)\n",
    "        print(\"<< Anfrage beendet\")\n",
    "\n",
    "    return jsonify({\"response\": response[\"choices\"][0][\"text\"].strip()})\n",
    "\n",
    "@app.route('/cometai/stream', methods=['POST'])\n",
    "def cometai_stream():\n",
    "    data = request.get_json()\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    history = data.get(\"history\", [])\n",
    "\n",
    "    ws_info = \"\"\n",
    "    if needs_websearch_simple(prompt):\n",
    "        print(\"Ich suche gerade im Web...\")\n",
    "        search_results = search_web(prompt)\n",
    "        ws_info = \"\\n\".join(search_results)\n",
    "\n",
    "    full_prompt = build_prompt(history, prompt, ws_info)\n",
    "    print(\"--- FINAL PROMPT ---\")\n",
    "    print(full_prompt)\n",
    "\n",
    "\n",
    "    def generate():\n",
    "        with model_lock:\n",
    "            for chunk in llm(prompt, max_tokens=512, stream=True, temperature=0.7, repeat_penalty=1.1, stop=[\"<|user|>:\", \"<|assistant|>:\", \"<|system|>:\"]):\n",
    "                yield chunk['choices'][0]['text']\n",
    "\n",
    "    return Response(stream_with_context(generate()), content_type='text/plain')\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74d8d50-4120-4c55-9825-71041e96c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "975f71dc-bd47-4767-bcf3-48895fc4d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Du:\n",
      "\t Do you speak English?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CometAI:\n",
      "\t Ja, ich kann Englisch sprechen. Aber ich antworte in diesem Chat immer auf Deutsch, wie du gesehen hast.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Du:\n",
      "\t You have to speak English with me!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CometAI:\n",
      "\t Entschuldigung, ich versuchte, auf Deutsch zu antworten. Lassen Sie mich bitte fortan nur auf Englisch sprechen.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Du:\n",
      "\t OKay sir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CometAI:\n",
      "\t Gerne geschehen! Wie kann ich Ihnen helfen heute?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m history = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     msg = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDu:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\CometAI\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1260\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1258\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\CometAI\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1303\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1304\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "while True:\n",
    "    msg = input(\"Du:\\n\\t\")\n",
    "    if msg.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "\n",
    "    ws_info = \"\"\n",
    "    if needs_websearch_simple(msg):\n",
    "        print(\"Ich suche gerade im Web...\")\n",
    "        search_results = search_web(msg)\n",
    "        ws_info = \"\\n\".join(search_results)\n",
    "\n",
    "    prompt = build_prompt(history, msg, ws_info)\n",
    "\n",
    "    print(\"CometAI:\\n\\t\", end=\"\", flush=True)\n",
    "\n",
    "    # === STREAMING-ANTWORT START ===\n",
    "    response_chunks = []\n",
    "    for chunk in llm(prompt, max_tokens=512, stream=True, temperature=0.7, repeat_penalty=1.1, stop=[\"<|user|>:\", \"<|assistant|>:\", \"<|system|>:\"]):\n",
    "        token = chunk[\"choices\"][0][\"text\"]\n",
    "        response_chunks.append(token)\n",
    "        print(token, end=\"\", flush=True)\n",
    "    print()  # Neue Zeile nach der Antwort\n",
    "\n",
    "    full_response = \"\".join(response_chunks).strip()\n",
    "    history.append({\"role\": \"assistant\", \"content\": full_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c526e-b71b-4446-906f-1441968f91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL KONFIGURATION ===\n",
    "llm = Llama(\n",
    "    model_path=\"models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",  # ← Pfad anpassen!\n",
    "    n_ctx=2048,\n",
    "    n_gpu_layers=50,        # GPU-Nutzung (so viel wie deine GPU erlaubt)\n",
    "    use_mlock=True,         # RAM-Pinning\n",
    "    use_mmap=True,          # schneller Zugriff\n",
    "    verbose=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
